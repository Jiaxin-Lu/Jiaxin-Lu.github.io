---
layout: post
title: UGG Unified Generative Grasping
author: Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang, Qixing Huang, Gang Hua
cover: /assets/img/ugg.png
cover_display: True
status: In Submission, Under Review
abstract: We introduce a unified diffusion-based dexterous grasp generation model, UGG. Our all-transformer architecture unifies the information from the object, the hand, and the contacts. A proposed lightweight discriminator, benifiting from simulated data, pushes for a high success rate while preserving high diversity. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research.
pdf: https://arxiv.org/abs/2311.16917
link: https://jiaxin-lu.github.io/ugg/
---

## Abstract

Automated assembly of 3D fractures is essential in orthopedics, archaeology, and our daily life. This paper presents Jigsaw, a novel framework for assembling physically broken 3D objects from multiple pieces. Our approach leverages hierarchical features of global and local geometry to match and align the fracture surfaces. Our framework consists of three major components: (1) surface segmentation to separate fracture and original parts, (2) multi-parts matching to find correspondences among fracture surface points, and (3) robust global alignment to recover the global poses of the pieces.
We show how to jointly learn segmentation and matching and seamlessly integrate feature matching and rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and achieve superior performance compared to state-of-the-art methods. Our method also generalizes well to diverse fracture modes, objects, and unseen instances. To the best of our knowledge, this is the first learning-based method designed specifically for 3D fracture assembly over multiple pieces.

![Pipeline](\assets\img\jigsaw.png)

[[arxiv](https://arxiv.org/abs/2305.17975)]


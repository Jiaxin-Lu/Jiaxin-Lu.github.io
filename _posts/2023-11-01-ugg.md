---
layout: post
title: "UGG: Unified Generative Grasping"
author: Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang, Qixing Huang, Gang Hua
cover: /assets/img/ugg.png
cover_display: True
status: In Submission, Under Review
abstract: We introduce a unified diffusion-based dexterous grasp generation model, UGG. Our all-transformer architecture unifies the information from the object, the hand, and the contacts. A proposed lightweight discriminator, benifiting from simulated data, pushes for a high success rate while preserving high diversity. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research.
pdf: https://arxiv.org/abs/2311.16917
link: https://jiaxin-lu.github.io/ugg/
---

## Abstract

Dexterous grasping aims to produce diverse grasping postures with a high grasping success rate. Regression-based methods that directly predict grasping parameters given the object may achieve a high success rate but often lack diversity. Generation-based methods that generate grasping postures conditioned on the object can often produce diverse grasping, but they are insufficient for high grasping success due to lack of discriminative information. To mitigate, we introduce a unified diffusion-based dexterous grasp generation model, dubbed the name UGG, which operates within the object point cloud and hand parameter spaces. Our all-transformer architecture unifies the information from the object, the hand, and the contacts, introducing a novel representation of contact points for improved contact modeling. The flexibility and quality of our model enable the integration of a lightweight discriminator, benefiting from simulated discriminative data, which pushes for a high success rate while preserving high diversity. Beyond grasp generation, our model can also generate objects based on hand information, offering valuable insights into object design and studying how the generative model perceives objects. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research.


[[arxiv](https://arxiv.org/abs/2305.17975)]

